{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Efficient Tricks\n",
    "## Code profiling for time usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit -r(runs)5 -n(loops)25 set(heroes)\n",
    "%timeit -r5 -n25 set(heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load module\n",
    "pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modele first\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#command to run the profiling\n",
    "%lprun -f convert_units convert_units(heroes, hts, wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code profiling for memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load module first, then since it requires code that need to be saved in a phycial file, so need to load it first\n",
    "pip install memory_profiler\n",
    "from hero_funcs import convert_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modele first\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#command to run the profiling\n",
    "%mprun -f convert_units convert_units(heroes, hts, wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## itertool, counter(), zip(), set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip will return a zip object and need to use * to unzip it to a list\n",
    "names_type1 = [*zip(names, primary_types)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collections.Counter() is faster than nested loop to count things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intertools contains good stuff for combination/permutation\n",
    "from itertools import combinations\n",
    "#this is an combination object, change number to change number in each combination\n",
    "combos_obj = combinations(pokemon, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to get intersection of 2 sets, all elements in both sets\n",
    "set.intersection()\n",
    "#all elements in one set but not the other\n",
    "set.difference()\n",
    "#all elements in exactly one set\n",
    "set.symmetric_difference()\n",
    "#all elemnts that are in both sets\n",
    "set.union\n",
    "#set is faster than list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eliminating loops with built in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map is faster than line coprehesion\n",
    "[sum(row) for row in rows]\n",
    "[*map(sum, rows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy also has built-in function that boost the effiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if loop can't be avoided, then move those one time calculation out of loop, or conversion out of loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas iterrows is faster than .iloc\n",
    "for i, row in df.iterrows()\n",
    "for i, row in df.itertuples()\n",
    "#itertuples is faster than iterrows since it return different object\n",
    "#iterrows run tuple, itertuples return named tuple, which used less overhead\n",
    "#use .apply() to prevent loop, like .map()\n",
    "#.apply(0 for column, 1 for row)\n",
    "# can use lambda\n",
    "textual_playoffs = rays_df.apply(lambda row: text_playoffs(row['Playoffs']), axis=1)\n",
    "print(textual_playoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['name'].values will return a np array and it allows us to directly calculate them with the whole data set instead \n",
    "#of row by row since we have broadcasting of numpy array, which will be much more faster than looping.\n",
    "df['new_name'] = df['name1'].values - df['name2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_perc_preds_loop = []\n",
    "\n",
    "# Use a loop and .itertuples() to collect each row's predicted win percentage\n",
    "for row in baseball_df.itertuples():\n",
    "    runs_scored = row.RS\n",
    "    runs_allowed = row.RA\n",
    "    win_perc_pred = predict_win_perc(runs_scored, runs_allowed)\n",
    "    win_perc_preds_loop.append(win_perc_pred)\n",
    "\n",
    "# Apply predict_win_perc to each row of the DataFrame\n",
    "win_perc_preds_apply = baseball_df.apply(lambda row: predict_win_perc(row['RS'], row['RA']), axis=1)\n",
    "\n",
    "# Calculate the win percentage predictions using NumPy arrays\n",
    "win_perc_preds_np = predict_win_perc(baseball_df['RS'].values, baseball_df['RA'].values)\n",
    "baseball_df['WP_preds'] = win_perc_preds_np\n",
    "print(baseball_df.head())\n",
    "\n",
    "#last method with np array is fastest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writting functions in python\n",
    "## docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it's the comment for describing the usage of the function, usually contains 5 parts.\n",
    "#start and end with \"\"\"\"\"\"\n",
    "#1description\n",
    "#2Args\n",
    "#3Returns\n",
    "#4Raises\n",
    "#\n",
    "def count_letter(content, letter):\n",
    "  \"\"\"Count the number of times `letter` appears in `content`.\n",
    "\n",
    "  Args:\n",
    "    content (str): The string to search.\n",
    "    letter (str): The letter to search for.\n",
    "\n",
    "  Returns:\n",
    "    int\n",
    "\n",
    "  # Add a section detailing what errors might be raised\n",
    "  Raises:\n",
    "    ValueError: If `letter` is not a one-character string.\n",
    "  \"\"\"\n",
    "  if (not isinstance(letter, str)) or len(letter) != 1:\n",
    "    raise ValueError('`letter` must be a single character string.')\n",
    "  return len([char for char in content if char == letter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "docstring = inspect.getdoc(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pass by assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass by assignment\n",
    "# use immuteable as function arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context manager\n",
    "with open('alice.txt')#contextmanager\n",
    "    as file:\n",
    "        \n",
    "with timer():\n",
    "  print('Numpy version')\n",
    "  process_with_numpy(image)\n",
    "#yield, which is used to run the commands within the context manager\n",
    "def in_dir(directory):\n",
    "  \"\"\"Change current working directory to `directory`,\n",
    "  allow the user to run some code, and change back.\n",
    "\n",
    "  Args:\n",
    "    directory (str): The path to a directory to work in.\n",
    "  \"\"\"\n",
    "  current_dir = os.getcwd()\n",
    "  os.chdir(directory)\n",
    "\n",
    "  # Add code that lets you handle errors\n",
    "  try:\n",
    "    yield\n",
    "  # Ensure the directory is reset,\n",
    "  # whether there was an error or not\n",
    "  finally:\n",
    "    os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function is an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the missing function references to the function map\n",
    "function_map = {\n",
    "  'mean': mean,\n",
    "  'std': std,\n",
    "  'minimum': minimum,\n",
    "  'maximum': maximum\n",
    "}\n",
    "\n",
    "data = load_data()\n",
    "print(data)\n",
    "\n",
    "func_name = get_user_input()\n",
    "\n",
    "# Call the chosen function and pass \"data\" as an argument\n",
    "function_map[func_name](data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local->global->builtin\n",
    "global is used to reach out to variable outside local\n",
    "nonlocal is used to reach out to variable outside local but in parent\n",
    "nonlocal variable is defined in the parent function but used in child function\n",
    "closures is nonlocal variable attached to a returned function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_a_func(arg1, arg2):\n",
    "  def new_func():\n",
    "    print('arg1 was {}'.format(arg1))\n",
    "    print('arg2 was {}'.format(arg2))\n",
    "  return new_func\n",
    "    \n",
    "my_func = return_a_func(2, 17)\n",
    "\n",
    "print(my_func.__closure__ is not None)\n",
    "print(len(my_func.__closure__) == 2)\n",
    "\n",
    "# Get the values of the variables in the closure\n",
    "closure_values = [\n",
    "  my_func.__closure__[i].cell_contents for i in range(2)\n",
    "]\n",
    "print(closure_values == [2, 17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@print_args\n",
    "def my_function(a, b, c):\n",
    "  print(a + b + c)\n",
    "\n",
    "my_function(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_return_type(func):\n",
    "  # Define wrapper(), the decorated function\n",
    "  def wrapper(*args, **kwyargs):\n",
    "    # Call the function being decorated\n",
    "    result = func(*args, **kwyargs)\n",
    "    print('{}() returned type {}'.format(\n",
    "      func.__name__, type(result)\n",
    "    ))\n",
    "    return result\n",
    "  # Return the decorated function\n",
    "  return wrapper\n",
    "  \n",
    "@print_return_type\n",
    "def foo(value):\n",
    "  return value\n",
    "  \n",
    "print(foo(42))\n",
    "print(foo([1, 2, 3]))\n",
    "print(foo({'a': 42}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(func):\n",
    "  def wrapper(*args, **kwargs):\n",
    "    wrapper.count += 1\n",
    "    # Call the function being decorated and return the result\n",
    "    return wrapper.count\n",
    "  wrapper.count = 0\n",
    "  # Return the new decorated function\n",
    "  return wrapper\n",
    "\n",
    "# Decorate foo() with the counter() decorator\n",
    "@counter\n",
    "def foo():\n",
    "  print('calling foo()')\n",
    "  \n",
    "foo()\n",
    "foo()\n",
    "\n",
    "print('foo() was called {} times.'.format(foo.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preserving docstrings when decorating functions\n",
    "# use functools wraps to protect doc in func instead of wrapper\n",
    "from functools import wraps\n",
    "\n",
    "def add_hello(func):\n",
    "  # Decorate wrapper() so that it keeps func()'s metadata\n",
    "  @wraps(func)\n",
    "  def wrapper(*args, **kwargs):\n",
    "    \"\"\"Print 'hello' and then call the decorated function.\"\"\"\n",
    "    print('Hello')\n",
    "    return func(*args, **kwargs)\n",
    "  return wrapper\n",
    "  \n",
    "@add_hello\n",
    "def print_sum(a, b):\n",
    "  \"\"\"Adds two numbers and prints the sum\"\"\"\n",
    "  print(a + b)\n",
    "  \n",
    "print_sum(10, 20)\n",
    "print(print_sum.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use duplicate.__wrapped__() to call the raw func without decorater\n",
    "@check_everything\n",
    "def duplicate(my_list):\n",
    "  \"\"\"Return a new list that repeats the input twice\"\"\"\n",
    "  return my_list + my_list\n",
    "\n",
    "t_start = time.time()\n",
    "duplicated_list = duplicate(list(range(50)))\n",
    "t_end = time.time()\n",
    "decorated_time = t_end - t_start\n",
    "\n",
    "t_start = time.time()\n",
    "# Call the original function instead of the decorated one\n",
    "duplicated_list = duplicate.__wrapped__(list(range(50)))\n",
    "t_end = time.time()\n",
    "undecorated_time = t_end - t_start\n",
    "\n",
    "print('Decorated time: {:.5f}s'.format(decorated_time))\n",
    "print('Undecorated time: {:.5f}s'.format(undecorated_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decorator that takes arguments\n",
    "def run_n_times(n):\n",
    "  \"\"\"Define and return a decorator\"\"\"\n",
    "  def decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "      for i in range(n):\n",
    "        func(*args, **kwargs)\n",
    "    return wrapper\n",
    "  return decorator\n",
    "# Modify the print() function to always run 20 times\n",
    "print = run_n_times(20)(print)\n",
    "\n",
    "print('What is happening?!?!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(*tags):\n",
    "  # Define a new decorator, named \"decorator\", to return\n",
    "  def decorator(func):\n",
    "    # Ensure the decorated function keeps its metadata\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "      # Call the function being decorated and return the result\n",
    "      return func(*args, **kwargs)\n",
    "    wrapper.tags = tags\n",
    "    return wrapper\n",
    "  # Return the new decorator\n",
    "  return decorator\n",
    "\n",
    "@tag('test', 'this is a tag')\n",
    "def foo():\n",
    "  pass\n",
    "\n",
    "print(foo.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns(return_type):\n",
    "  # Complete the returns() decorator\n",
    "  def decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "      result = func(*args, **kwargs)\n",
    "      assert(type(result) == return_type)\n",
    "      return result\n",
    "    return wrapper\n",
    "  return decorator\n",
    "  \n",
    "@returns(dict)\n",
    "def foo(value):\n",
    "  return value\n",
    "\n",
    "try:\n",
    "  print(foo([1,2,3]))\n",
    "except AssertionError:\n",
    "  print('foo() did not return a dict!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the pytest package\n",
    "import pytest\n",
    "\n",
    "# Import the function convert_to_int()\n",
    "from preprocessing_helpers import convert_to_int\n",
    "\n",
    "# Complete the unit test name by adding a prefix\n",
    "def test_on_string_with_one_comma():\n",
    "  # Complete the assert statement\n",
    "  assert convert_to_int('2,081')==2081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you get an AssertionError, this means the function has a bug and you should fix it. \n",
    "#If you get another exception, e.g. NameError, this means that something else is wrong with the\n",
    "#unit test code and you should fix it so that the assert statement can actually run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benefit of unit testing\n",
    "#reduced time\n",
    "#improved documentation\n",
    "#more trust\n",
    "#reduced downtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mutliple assert\n",
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                                )\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = 2\n",
    "    actual = split_into_training_and_testing_sets(example_argument)\n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
    "    # Write the assert statement checking testing array's number of rows\n",
    "    assert actual[1].shape[0] == expected_testing_array_num_rows, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special value, boundry value, normal value, bad value\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n",
    "    # Assign actual to the return value for the argument \"123\\n\"\n",
    "    actual = row_to_list(\"123\\n\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n",
    "    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
    "    actual = row_to_list(\"\\t4,567\\n\")\n",
    "    # Format the failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use a class to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a class to handle increasing test\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# Declare the test class\n",
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "    # Fill in with the correct mandatory argument\n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to run all tests in test folder, use pytest under test folder\n",
    "pytest -x to return when one test is failed\n",
    "#class only NodeIDofatestclass:<path to test module>::<test class name>\n",
    "#function under class only NodeIDofanunittest:<path to test module>::<test class name>::<unit test name>\n",
    "#RunsalltestswhosenodeIDmatchesthepaern.\n",
    "pytest -k \"pattern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exptet to fail\n",
    "# Add a reason for the expected failure\n",
    "@pytest.mark.xfail(reason='Using TDD, model_test() has not yet been implemented')\n",
    "class TestModelTest(object):\n",
    "    def test_on_linear_data(self):\n",
    "        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "        expected = 1.0\n",
    "        actual = model_test(test_input, 2.0, 1.0)\n",
    "        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
    "        assert actual == pytest.approx(expected), message\n",
    "        \n",
    "    def test_on_one_dimensional_array(self):\n",
    "        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            model_test(test_input, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip the fail\n",
    "# Import the sys module\n",
    "import sys\n",
    "\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    # Add a reason for skipping the test\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower\")\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                             ]\n",
    "                            )\n",
    "        actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "        assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showingreasonforskipping\n",
    "pytest -rs\n",
    "#Showingreasonforxfail\n",
    "pytest -rx\n",
    "#Showingreasonforbothskippedandxfail\n",
    "pytest -rsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a decorator to make this function a fixture\n",
    "@pytest.fixture\n",
    "def clean_data_file():\n",
    "    file_path = \"clean_data_file.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
    "    yield file_path\n",
    "    os.remove(file_path)\n",
    "    \n",
    "# Pass the correct argument so that the test can use the fixture\n",
    "def test_on_clean_file(clean_data_file):\n",
    "    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
    "    # Pass the clean data file path yielded by the fixture as the first argument\n",
    "    actual = get_data_as_numpy_array(clean_data_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tmpdir\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
    "def empty_file(tmpdir):\n",
    "    # Use the appropriate method to create an empty file in the temporary directory\n",
    "    file_path = tmpdir.join(\"empty.txt\")\n",
    "    open(file_path, \"w\").close()\n",
    "    yield file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mock\n",
    "#Mocking:testingfunctionsindependentlyofdependencies\n",
    "\n",
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                       side_effect=convert_to_int_bug_free)\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from visualization import get_plot_for_best_fit_line\n",
    "@pytest.mark.mpl_image_compare    \n",
    "# Under the hood baseline generation and comparison\n",
    "def test_plot_for_linear_data():    \n",
    "    slope = 2.0    \n",
    "    intercept = 1.0    \n",
    "    x_array = np.array([1.0, 2.0, 3.0])    \n",
    "    # Linear data set    \n",
    "    y_array = np.array([3.0, 5.0, 7.0])    \n",
    "    title = \"Test plot for linear data\"\n",
    "    return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the baseline image\n",
    "pytest --mpl-generate-path /home/repl/workspace/project/tests/visualization/baseline -k \"test_plot_for_almost_linear_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the test\n",
    "pytest -k \"test_plot_for_linear_data\" --mpl\n",
    "#Reading failure report\n",
    "pytest -k \"test_plot_for_linear_data\" --mpl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
