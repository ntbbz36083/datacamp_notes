{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DAG object\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 1, 14),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "etl_dag = DAG('example_etl', default_args=default_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another Python task\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json','outputfile':'parsedfile.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "# Import the Operator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow sensor/executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airflow sensor, airflow.sensors.base_sensor_operator\n",
    "#mode='poke'-The default mode,run repeatedly\n",
    "#mode='reschedule'-Give up task slot and try again later\n",
    "#poke_interval-How ofen to wait between checks\n",
    "#timeout-How long to wait before failing task\n",
    "#Sequential Executor \n",
    "only run 1 task per time, use for debug\n",
    "#LocalExecutor \n",
    "Parallelism\n",
    "#CeleryExecutor\n",
    "most powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## debugging\n",
    "1.DAG won't run on schedule\n",
    "run airflow scheduler to fix scheduler\n",
    "\n",
    "2.DAG won't run on schedule\n",
    "At least one schedule_interval hasn't passed.\n",
    "Not enough tasks free within the executor to run\n",
    "Change executor type\n",
    "\n",
    "3.DAG won't load\n",
    "Syntax errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Defining SLAs\n",
    "1.Using the 'sla' argument on the task\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla=timedelta(hours=3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)\n",
    "2.On the default_args dictionary\n",
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 2, 20),\n",
    "  'sla': timedelta(minutes=30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval='@None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=['monthly_report.pdf'],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': ['airflowalerts@datacamp.com','airflowadmin@datacamp.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_success':True\n",
    "}\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the templated command to handle a\n",
    "# second argument called filename.\n",
    "templated_command = \"\"\"\n",
    "  bash cleandata.sh {{ ds_nodash }} {{params.filename}}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to pass the new argument\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'salesdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
    "                           bash_command=templated_command,\n",
    "                          params={'filename': 'supportdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "                           \n",
    "# Set the operator dependencies\n",
    "clean_task >> clean_task2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#email template\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content\n",
    "html_email_str = \"\"\"\n",
    "Date: {{ ds }}\n",
    "Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "                default_args={'start_date': datetime(2020, 4, 15)},\n",
    "                schedule_interval='@weekly')\n",
    "                \n",
    "email_task = EmailOperator(task_id='email_task',\n",
    "                           to='testuser@datacamp.com',\n",
    "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "                           html_content=html_email_str,\n",
    "                           params={'username': 'testemailuser'},\n",
    "                           dag=email_dag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch\n",
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_dag >> current_year_task\n",
    "branch_dag >> new_year_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "# Import the needed operators\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import date, datetime\n",
    "\n",
    "def process_data(**context):\n",
    "  file = open('/home/repl/workspace/processed_data.tmp', 'w')\n",
    "  file.write(f'Data processed on {date.today()}')\n",
    "  file.close()\n",
    "\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2020,4,1)})\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=5,\n",
    "                    timeout=15,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2019,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "\n",
    "no_email_task = DummyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable=check_weekend,\n",
    "                                   provide_context=True,\n",
    "                                   dag=dag)\n",
    "\n",
    "    \n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PLATFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:5000\"\n",
    "\n",
    "# Fill in the correct API key\n",
    "api_key = \"scientist007\"\n",
    "\n",
    "# Create the web API’s URL\n",
    "authenticated_endpoint = \"{}/{}\".format(endpoint, api_key)\n",
    "\n",
    "# Get the web API’s reply to the endpoint\n",
    "api_response = requests.get(authenticated_endpoint).json()\n",
    "pprint.pprint(api_response)\n",
    "\n",
    "# Create the API’s endpoint for the shops\n",
    "shops_endpoint = \"{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"shops\")\n",
    "shops = requests.get(shops_endpoint).json()\n",
    "print(shops)\n",
    "\n",
    "# Create the API’s endpoint for items of the shop starting with a \"D\"\n",
    "items_of_specific_shop_URL = \"{}/{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"items\", \"DM\")\n",
    "products_of_shop = requests.get(items_of_specific_shop_URL).json()\n",
    "pprint.pprint(products_of_shop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the convenience function to query the API\n",
    "tesco_items = retrieve_products(\"Tesco\")\n",
    "\n",
    "singer.write_schema(stream_name=\"products\", schema=schema,\n",
    "                    key_properties=[])\n",
    "\n",
    "# Write a single record to the stream, that adheres to the schema\n",
    "singer.write_record(stream_name=\"products\", \n",
    "                    record={**tesco_items[0], \"store_name\": \"Tesco\"})\n",
    "\n",
    "for shop in requests.get(SHOPS_URL).json()[\"shops\"]:\n",
    "    # Write all of the records that you retrieve from the API\n",
    "    singer.write_records(\n",
    "      stream_name=\"products\", # Use the same stream name that you used in the schema\n",
    "      records=({**item, \"store_name\": shop}\n",
    "               for item in retrieve_products(shop))\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pySpark handing bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "  StructField(\"brand\", StringType(), nullable=False),\n",
    "  StructField(\"model\", StringType(), nullable=False),\n",
    "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
    "  StructField(\"comfort\", ByteType(), nullable=True)\n",
    "])\n",
    "\n",
    "better_df = (spark\n",
    "             .read\n",
    "             .options(header=\"true\")\n",
    "             # Pass the predefined schema to the Reader\n",
    "             .schema(schema)\n",
    "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "pprint(better_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Add/relabel the column\n",
    "categorized_ratings = ratings.withColumn(\n",
    "    \"comfort\",\n",
    "    # Express the condition in terms of column operations\n",
    "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
    "\n",
    "categorized_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select the columns and rename the \"absorption_rate\" column\n",
    "result = ratings.select([col(\"brand\"),\n",
    "                       col(\"model\"),\n",
    "                       col(\"absorption_rate\").alias('absorbency')])\n",
    "\n",
    "# Show only unique values\n",
    "result.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
    "\n",
    "aggregated = (purchased\n",
    "              # Group rows by 'Country'\n",
    "              .groupBy(col('Country'))\n",
    "              .agg(\n",
    "                # Calculate the average salary per group and rename\n",
    "                avg('Salary').alias('average_salary'),\n",
    "                # Calculate the standard deviation per group\n",
    "                stddev_samp('Salary'),\n",
    "                # Retain the highest salary per group and rename\n",
    "                sfmax('Salary').alias('highest_salary')\n",
    "              )\n",
    "             )\n",
    "\n",
    "aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
    "\n",
    "# Create a tuple of records\n",
    "data = (\n",
    "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
    "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
    "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
    ")\n",
    "\n",
    "# Create a DataFrame from these records\n",
    "frame = spark.createDataFrame(data)\n",
    "frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use airflow to do the similar job\n",
    "config = os.path.join(os.environ[\"AIRFLOW_HOME\"], \n",
    "                      \"scripts\",\n",
    "                      \"configs\", \n",
    "                      \"data_lake.conf\")\n",
    "\n",
    "ingest = BashOperator(\n",
    "  # Assign a descriptive id\n",
    "  task_id=\"ingest_data\", \n",
    "  # Complete the ingestion pipeline\n",
    "  bash_command=\"tap-marketing-api | target-csv --config %s\" % config,\n",
    "  dag=dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the operator\n",
    "from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator\n",
    "\n",
    "# Set the path for our files.\n",
    "entry_point = os.path.join(os.environ[\"AIRFLOW_HOME\"], \"scripts\", \"clean_ratings.py\")\n",
    "dependency_path = os.path.join(os.environ[\"AIRFLOW_HOME\"], \"dependencies\", \"pydiaper.zip\")\n",
    "\n",
    "with DAG('data_pipeline', start_date=datetime(2019, 6, 25),\n",
    "         schedule_interval='@daily') as dag:\n",
    "  \t# Define task clean, running a cleaning job.\n",
    "    clean_data = SparkSubmitOperator(\n",
    "        application=entry_point, \n",
    "        py_files=dependency_path,\n",
    "        task_id='clean_data',\n",
    "        conn_id='spark_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_args = {\"py_files\": dependency_path,\n",
    "              \"conn_id\": \"spark_default\"}\n",
    "# Define ingest, clean and transform job.\n",
    "with dag:\n",
    "    ingest = BashOperator(task_id='Ingest_data', bash_command='tap-marketing-api | target-csv --config %s' % config)\n",
    "    clean = SparkSubmitOperator(application=clean_path, task_id='clean_data', **spark_args)\n",
    "    insight = SparkSubmitOperator(application=transform_path, task_id='show_report', **spark_args)\n",
    "    \n",
    "    # set triggering sequence\n",
    "    ingest >> clean >> insight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
